{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Fine-tuning GPT-2 for text classification\n",
    "Stefan/Yuzhao Heng\n",
    "Since Wed. Feb. 9th, 2022\n",
    "\n",
    "\n",
    "Reproduce the results in paper [Zero-shot Text Classification With Generative Language Models](https://arxiv.org/abs/1912.10165),\n",
    "since the authors didn't release the code.\n",
    "\n",
    "Serve as infrastructure and baseline for project on efficient and accurate encoder for text classification with many labels.\n",
    "\n",
    "\n",
    "## Setup\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig, GPT2Tokenizer\n",
    "from datasets import load_dataset\n",
    "from icecream import ic\n",
    "\n",
    "\n",
    "from util import *\n",
    "\n",
    "\n",
    "transformers.set_seed(config('random-seed'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Prep Model & Dataset\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset ag_news (/Users/stefanh/.cache/huggingface/datasets/ag_news/default/0.0.0/bc2bcb40336ace1a0374767fc29bb0296cdaf8a6da7298436239c54d79180548)\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ddba3e1ebe2341ec997bc0f420191e7f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| tokenizer: PreTrainedTokenizerFast(name_or_path='gpt2', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>'})\n",
      "ic| AutoConfig.from_pretrained(model_nm): GPT2Config {\n",
      "                                            \"activation_function\": \"gelu_new\",\n",
      "                                            \"architectures\": [\n",
      "                                              \"GPT2LMHeadModel\"\n",
      "                                            ],\n",
      "                                            \"attn_pdrop\": 0.1,\n",
      "                                            \"bos_token_id\": 50256,\n",
      "                                            \"embd_pdrop\": 0.1,\n",
      "                                            \"eos_token_id\": 50256,\n",
      "                                            \"initializer_range\": 0.02,\n",
      "                                            \"layer_norm_epsilon\": 1e-05,\n",
      "                                            \"model_type\": \"gpt2\",\n",
      "                                            \"n_ctx\": 1024,\n",
      "                                            \"n_embd\": 768,\n",
      "                                            \"n_head\": 12,\n",
      "                                            \"n_inner\": null,\n",
      "                                            \"n_layer\": 12,\n",
      "                                            \"n_positions\": 1024,\n",
      "                                            \"resid_pdrop\": 0.1,\n",
      "                                            \"scale_attn_weights\": true,\n",
      "                                            \"summary_activation\": null,\n",
      "                                            \"summary_first_dropout\": 0.1,\n",
      "                                            \"summary_proj_to_labels\": true,\n",
      "                                            \"summary_type\": \"cls_index\",\n",
      "                                            \"summary_use_proj\": true,\n",
      "                                            \"task_specific_params\": {\n",
      "                                              \"text-generation\": {\n",
      "                                                \"do_sample\": true,\n",
      "                                                \"max_length\": 50\n",
      "                                              }\n",
      "                                            },\n",
      "                                            \"transformers_version\": \"4.11.3\",\n",
      "                                            \"use_cache\": true,\n",
      "                                            \"vocab_size\": 50257\n",
      "                                          }\n",
      "ic| tokenizer.tokenize(s): ['Which',\n",
      "                            'Ġof',\n",
      "                            'Ġthese',\n",
      "                            'Ġchoices',\n",
      "                            'Ġbest',\n",
      "                            'Ġdescribes',\n",
      "                            'Ġthe',\n",
      "                            'Ġfollowing',\n",
      "                            'Ġdocument',\n",
      "                            '?',\n",
      "                            'Ġ:',\n",
      "                            'ĠâĢ',\n",
      "                            'ľ',\n",
      "                            'ĠA',\n",
      "                            'Ġpool',\n",
      "                            'ĠFor',\n",
      "                            'ĠAll',\n",
      "                            'ĠB',\n",
      "                            'odies',\n",
      "                            'ĠâĢ',\n",
      "                            'Ŀ',\n",
      "                            'Ġ,',\n",
      "                            'ĠâĢ',\n",
      "                            'ľ',\n",
      "                            'ĠLaw',\n",
      "                            'makers',\n",
      "                            'Ġsay',\n",
      "                            'Ġthey',\n",
      "                            'âĢ',\n",
      "                            'Ļ',\n",
      "                            'd',\n",
      "                            'Ġtake',\n",
      "                            'Ġpay',\n",
      "                            'Ġcut',\n",
      "                            ',',\n",
      "                            'Ġbut',\n",
      "                            'Ġthey',\n",
      "                            'Ġcan',\n",
      "                            'âĢ',\n",
      "                            'Ļ',\n",
      "                            't',\n",
      "                            'ĠâĢ',\n",
      "                            'Ŀ',\n",
      "                            'Ġ,',\n",
      "                            'ĠâĢ',\n",
      "                            'ľ',\n",
      "                            'ĠRaiders',\n",
      "                            'âĢ',\n",
      "                            'Ļ',\n",
      "                            'ĠG',\n",
      "                            'are',\n",
      "                            'on',\n",
      "                            'ĠCon',\n",
      "                            'ley',\n",
      "                            'Ġfaces',\n",
      "                            'Ġcivil',\n",
      "                            'Ġsuit',\n",
      "                            'ĠâĢ',\n",
      "                            'Ŀ',\n",
      "                            'Ġ,',\n",
      "                            'ĠâĢ',\n",
      "                            'ľ',\n",
      "                            'ĠPro',\n",
      "                            'li',\n",
      "                            'ï',\n",
      "                            '¬',\n",
      "                            'ģ',\n",
      "                            'c',\n",
      "                            'Ġcyber',\n",
      "                            'criminal',\n",
      "                            'Ġsuspected',\n",
      "                            'Ġof',\n",
      "                            'Ġspreading',\n",
      "                            'Ġransomware',\n",
      "                            'Ġarrested',\n",
      "                            'Ġby',\n",
      "                            'ĠPolish',\n",
      "                            'ĠPolice',\n",
      "                            'Ġ[',\n",
      "                            'Euro',\n",
      "                            'pol',\n",
      "                            ']',\n",
      "                            'ĠâĢ',\n",
      "                            'Ŀ']\n"
     ]
    },
    {
     "data": {
      "text/plain": "['Which',\n 'Ġof',\n 'Ġthese',\n 'Ġchoices',\n 'Ġbest',\n 'Ġdescribes',\n 'Ġthe',\n 'Ġfollowing',\n 'Ġdocument',\n '?',\n 'Ġ:',\n 'ĠâĢ',\n 'ľ',\n 'ĠA',\n 'Ġpool',\n 'ĠFor',\n 'ĠAll',\n 'ĠB',\n 'odies',\n 'ĠâĢ',\n 'Ŀ',\n 'Ġ,',\n 'ĠâĢ',\n 'ľ',\n 'ĠLaw',\n 'makers',\n 'Ġsay',\n 'Ġthey',\n 'âĢ',\n 'Ļ',\n 'd',\n 'Ġtake',\n 'Ġpay',\n 'Ġcut',\n ',',\n 'Ġbut',\n 'Ġthey',\n 'Ġcan',\n 'âĢ',\n 'Ļ',\n 't',\n 'ĠâĢ',\n 'Ŀ',\n 'Ġ,',\n 'ĠâĢ',\n 'ľ',\n 'ĠRaiders',\n 'âĢ',\n 'Ļ',\n 'ĠG',\n 'are',\n 'on',\n 'ĠCon',\n 'ley',\n 'Ġfaces',\n 'Ġcivil',\n 'Ġsuit',\n 'ĠâĢ',\n 'Ŀ',\n 'Ġ,',\n 'ĠâĢ',\n 'ľ',\n 'ĠPro',\n 'li',\n 'ï',\n '¬',\n 'ģ',\n 'c',\n 'Ġcyber',\n 'criminal',\n 'Ġsuspected',\n 'Ġof',\n 'Ġspreading',\n 'Ġransomware',\n 'Ġarrested',\n 'Ġby',\n 'ĠPolish',\n 'ĠPolice',\n 'Ġ[',\n 'Euro',\n 'pol',\n ']',\n 'ĠâĢ',\n 'Ŀ']"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dnm = 'ag_news'\n",
    "dset = load_dataset(dnm)\n",
    "# ic(dset)\n",
    "\n",
    "\n",
    "# Models used in the paper, parameter sizes: 124M, 355M; Discrepancy with Open AI reported numbers?\n",
    "model_nms = dict(small='gpt2', large='gpt2-medium')\n",
    "model_nm = model_nms['small']\n",
    "model = AutoModel.from_pretrained(model_nm)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_nm)\n",
    "# ic(tokenizer)\n",
    "\n",
    "# def model_param_size(m: torch.nn.Module) -> str:\n",
    "#    return fmt_num(sum(p.numel() for p in m.parameters()))\n",
    "#\n",
    "# ic(model_param_size(model))\n",
    "# ic(model_param_size(AutoModel.from_pretrained('gpt2-medium')))\n",
    "\n",
    "\n",
    "# ic(AutoConfig.from_pretrained(model_nm))\n",
    "\n",
    "\n",
    "\n",
    "# s = \"Which of these choices best describes the following document? : “ A pool For All Bodies ” , “ Lawmakers say they’d take pay cut, but they can’t ” , “ Raiders’ Gareon Conley faces civil suit ” , “ Proliﬁc cybercriminal suspected of spreading ransomware arrested by Polish Police [Europol] ”\"\n",
    "# # ic(tokenizer(s))\n",
    "# ic(tokenizer.tokenize(s))\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}